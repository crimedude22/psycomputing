{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Files\n",
    "\n",
    "It is pretty rare that you type all the data you want into Python. Most of the time, you will be getting data from different sources. Likewise, you will need to be able to save your data so you can use it in other programs (Excel, SPSS, R, etc.). This section will teach you the basics of reading and writing files so you can move data between Python and these other sources. \n",
    "\n",
    "### Common File Formats\n",
    "There are various formats that people use for saving data. The easiest thing to deal with is plain text, because it can be used everywhere. BUT, there are different conventions people use for saving data in plain text. \n",
    "\n",
    "* The most common is probably **Comma Separated Values (.csv)** files. These are for saving tabular data. The rule is very simple: each row of the data is on a separate line, and each column of data is separated by a comma character \",\"\n",
    "\n",
    "* Another common format is a **tab delimited** file. These usually have the .txt or .dat file extension. They are like csv files, except each column is separated by a tab character (like when you hit tab on the keyboard). In plain text files, tab is represented as `\\t`. When you open a file in Notepad, Word, etc. then it hides these characters from you, but they are still there!\n",
    "\n",
    "* A popular format for passing data over the internet is **Javascript Object Notation (JSON)**. These are still plain text, but the information is stored a lot like a Python dictionary. There are named fields with matching values. They can also store lists. The benefit of this format is that it can handle data that is not a simple spreadsheet. \n",
    "    * Think about the David Bowie dataset-- you have a single album with a single title and year, then a track list that contains several titles. This can easily be represented in a JSON file. \n",
    "    \n",
    "* Another format that's popular on the web is **XML** or **HTML**. Again, this is plain text, but information can be represented hierarchically. The basic rule is that different types of information are surrounded by \"tags\". Each tag is inside of brackets `<tag>` and has a matching closing tag with a slash `</tag>`. For instance, I could organize the music information like this: \n",
    "\n",
    "```xml\n",
    "<Artist>\n",
    "    <name>David Bowie</name>\n",
    "    <album>Heroes\n",
    "        <year>1977</year>\n",
    "        <tracks>\n",
    "            <song>Heroes</song>\n",
    "            <song>Beauty and the Beast</song>\n",
    "            ...\n",
    "        </tracks>\n",
    "    </album>\n",
    "<Artist>    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a Look\n",
    "\n",
    "* First let's look at some data. Navigate to the folder **R:\\Class_Data\\datasets\\sample_data**, and double-click on the \"sample_data.csv\" file. This will open it in Excel. That's because Excel knows how to read csv files. Notice that it looks like a normal excel sheet. The data are from a set of baby names that we will use later. There are 3 columns of data: a name, a gender (m/f) and the number of occurrences of that name in the year 2012. \n",
    "\n",
    "    * Now open the file using the program Notepad++ (open the program first, then drag the file into it). \n",
    "\n",
    "    * Notice how in Excel it displays things in different columns, but in Notepad++ you just see commas between values. \n",
    "\n",
    "* Now do the same thing with sample_data.txt. How is this different? \n",
    "\n",
    "\n",
    "### Loading files into Python\n",
    "\n",
    "There are 2 basic steps to loading or saving files. For the first step, you open the file (or create it, if it doesn't exist yet) and you create a **file handle**. This is just a variable, but it tells Python how to find the file on your hard drive. \n",
    "\n",
    "Then you use other functions to read or write information to/from the file. This varies based on the file type and what you want to do with it. \n",
    "\n",
    "First see how we open a file below, using the `open` function. We call our file handle `f`. \n",
    "\n",
    "The second argument to `open` is the *mode*. This tells Python what we want to do with the file. \n",
    "\n",
    "* 'r' means we will only read from the file\n",
    "* 'w' means we will read *and* write to the file\n",
    "    * if the file exists already, it is erased!\n",
    "* 'a' (think \"append\") opens it for reading and writing, BUT any writing is appended to the end of the file if it already exists. \n",
    "\n",
    "* you may notice code where people specify 'w+' or 'a+'. These are subtle differences that you probably don't need to know about. You can get more detail [here](http://stackoverflow.com/questions/1466000/python-open-built-in-function-difference-between-modes-a-a-w-w-and-r) though. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../datasets/sample_data/sample_data.csv','r') #open the file for reading\n",
    "\n",
    "#do some stuff with the file\n",
    "\n",
    "f.close() #close the file when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of uneventful, right? Let's do something else. Let's use the `readlines` function to read in all lines of the file, which are stored in a list. Take a look at `content` as well as the first 10 elements of it. How is the information stored? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../datasets/sample_data/sample_data.csv','r') #open the file for reading\n",
    "\n",
    "content = f.readlines()\n",
    "content = content[:10] #let's just grab the first 10 lines\n",
    "\n",
    "\n",
    "f.close() #close the file when you're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can loop through `content` and remove the newline characters (`\\r\\n`) from each element using `strip`. Save the result back into the `content` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next take each line of `content` and split it into separate columns based on the delimiter, which is comma \",\". Just print the result. Notice how many elements are in each list now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `with` function\n",
    "\n",
    "Notice in the example above, we have to call f.close() when we're done with the file. If we don't do this, it takes up memory and can cause other problems in your script. You only want the files open as long as you need them. If you do a complicated operation with your file, it is easy to forget to do this. \n",
    "\n",
    "That's why they created the `with`...`as` function. It works a lot like a loop, but it just specifies what you do to a file. When it's done, it closes the file for you, so you don't have to do f.close().\n",
    "\n",
    "See the example below. Notice how it has the colon an lines are indented, just like in an `if` statment or a `for` loop. In English, the statement reads like this: \n",
    "\n",
    "\"With this file that I'm opening, call it `f`, do stuff to it (readlines), then close it when you're done.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/sample_data/sample_data.csv','r') as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "print content[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a cleaner and more concise way to open your files, and you don't have to remember to close the file when you're done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The csv package\n",
    "\n",
    "Since csv files are so common, Python has a `csv` package for dealing with these files. In this example, we read in a file row-by-row (instead of all-at-once with `readlines`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sophia', 'F', '22267']\n",
      "['Emma', 'F', '20902']\n",
      "['Isabella', 'F', '19058']\n",
      "['Olivia', 'F', '17277']\n",
      "['Ava', 'F', '15512']\n",
      "['Emily', 'F', '13619']\n",
      "['Abigail', 'F', '12662']\n",
      "['Mia', 'F', '11998']\n",
      "['Madison', 'F', '11374']\n",
      "['Elizabeth', 'F', '9674']\n",
      "['Chloe', 'F', '9641']\n",
      "['Ella', 'F', '9177']\n",
      "['Avery', 'F', '8309']\n",
      "['Addison', 'F', '8165']\n",
      "['Aubrey', 'F', '8037']\n",
      "['Lily', 'F', '7945']\n",
      "['Natalie', 'F', '7885']\n",
      "['Sofia', 'F', '7820']\n",
      "['Charlotte', 'F', '7468']\n",
      "['Zoey', 'F', '7452']\n",
      "['Grace', 'F', '7359']\n",
      "['Hannah', 'F', '7261']\n",
      "['Amelia', 'F', '7235']\n",
      "['Harper', 'F', '7179']\n",
      "['Lillian', 'F', '7143']\n",
      "['Samantha', 'F', '6913']\n",
      "['Evelyn', 'F', '6869']\n",
      "['Victoria', 'F', '6849']\n",
      "['Brooklyn', 'F', '6769']\n",
      "['Zoe', 'F', '6435']\n",
      "['Layla', 'F', '6251']\n",
      "['Hailey', 'F', '5901']\n",
      "['Leah', 'F', '5758']\n",
      "['Kaylee', 'F', '5602']\n",
      "['Anna', 'F', '5600']\n",
      "['Aaliyah', 'F', '5495']\n",
      "['Gabriella', 'F', '5487']\n",
      "['Allison', 'F', '5415']\n",
      "['Nevaeh', 'F', '5356']\n",
      "['Alexis', 'F', '5337']\n",
      "['Audrey', 'F', '5291']\n",
      "['Savannah', 'F', '5176']\n",
      "['Sarah', 'F', '5167']\n",
      "['Alyssa', 'F', '5078']\n",
      "['Claire', 'F', '4941']\n",
      "['Taylor', 'F', '4852']\n",
      "['Riley', 'F', '4816']\n",
      "['Camila', 'F', '4789']\n",
      "['Arianna', 'F', '4708']\n",
      "['Ashley', 'F', '4691']\n",
      "['Brianna', 'F', '4624']\n",
      "['Sophie', 'F', '4566']\n",
      "['Peyton', 'F', '4464']\n",
      "['Bella', 'F', '4332']\n",
      "['Khloe', 'F', '4304']\n",
      "['Genesis', 'F', '4292']\n",
      "['Alexa', 'F', '4288']\n",
      "['Serenity', 'F', '4209']\n",
      "['Kylie', 'F', '4164']\n",
      "['Aubree', 'F', '4039']\n",
      "['Scarlett', 'F', '4039']\n",
      "['Stella', 'F', '3994']\n",
      "['Maya', 'F', '3961']\n",
      "['Katherine', 'F', '3945']\n",
      "['Julia', 'F', '3872']\n",
      "['Lucy', 'F', '3803']\n",
      "['Madelyn', 'F', '3791']\n",
      "['Autumn', 'F', '3765']\n",
      "['Makayla', 'F', '3760']\n",
      "['Kayla', 'F', '3755']\n",
      "['Mackenzie', 'F', '3750']\n",
      "['Lauren', 'F', '3654']\n",
      "['Gianna', 'F', '3615']\n",
      "['Ariana', 'F', '3572']\n",
      "['Faith', 'F', '3520']\n",
      "['Alexandra', 'F', '3478']\n",
      "['Melanie', 'F', '3462']\n",
      "['Sydney', 'F', '3449']\n",
      "['Caroline', 'F', '3412']\n",
      "['Naomi', 'F', '3411']\n",
      "['Bailey', 'F', '3399']\n",
      "['Morgan', 'F', '3392']\n",
      "['Ellie', 'F', '3382']\n",
      "['Kennedy', 'F', '3379']\n",
      "['Jasmine', 'F', '3367']\n",
      "['Eva', 'F', '3355']\n",
      "['Skylar', 'F', '3337']\n",
      "['Kimberly', 'F', '3307']\n",
      "['Violet', 'F', '3270']\n",
      "['Molly', 'F', '3251']\n",
      "['Aria', 'F', '3226']\n",
      "['Jocelyn', 'F', '3224']\n",
      "['Trinity', 'F', '3224']\n",
      "['London', 'F', '3192']\n",
      "['Lydia', 'F', '3160']\n",
      "['Madeline', 'F', '3157']\n",
      "['Reagan', 'F', '3083']\n",
      "['Piper', 'F', '3056']\n",
      "['Annabelle', 'F', '3046']\n",
      "['Andrea', 'F', '3041']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('./datasets/sample_data/sample_data.csv','r') as f:\n",
    "    csvreader = csv.reader(f,delimiter=',')\n",
    "    \n",
    "    for row in csvreader:\n",
    "        print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we first make a variable `csvreader` using `csv.reader`. We can then loop through each row using this variable. See how it already splits up our data into a list with 3 elements? This saves us from doing `strip` and `split`. \n",
    "\n",
    "Now try the same thing with our tab-delimited file `sample_data.txt`. Even though this isn't a .csv file, it's OK. The package is made for reading any delimited file. The only difference is that the delimiter is a tab (\\t) instead of a comma (,). \n",
    "\n",
    "Notice that we add a \"U\" to the mode when we call `open`. This enables \"universal newline mode\". This has to do with the way that Windows and Mac sometimes use different characters to specify a new line, which can cause problems in particular scenarios. I don't expect you to remember this. You'll know if you need it because you'll get an error that says: \n",
    "\n",
    "\"Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?\"\n",
    "\n",
    "So, if you use \"r\" and you get that error, try \"rU\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sophia\\tF\\t22267']\n",
      "['Emma\\tF\\t20902']\n",
      "['Isabella\\tF\\t19058']\n",
      "['Olivia\\tF\\t17277']\n",
      "['Ava\\tF\\t15512']\n",
      "['Emily\\tF\\t13619']\n",
      "['Abigail\\tF\\t12662']\n",
      "['Mia\\tF\\t11998']\n",
      "['Madison\\tF\\t11374']\n",
      "['Elizabeth\\tF\\t9674']\n",
      "['Chloe\\tF\\t9641']\n",
      "['Ella\\tF\\t9177']\n",
      "['Avery\\tF\\t8309']\n",
      "['Addison\\tF\\t8165']\n",
      "['Aubrey\\tF\\t8037']\n",
      "['Lily\\tF\\t7945']\n",
      "['Natalie\\tF\\t7885']\n",
      "['Sofia\\tF\\t7820']\n",
      "['Charlotte\\tF\\t7468']\n",
      "['Zoey\\tF\\t7452']\n",
      "['Grace\\tF\\t7359']\n",
      "['Hannah\\tF\\t7261']\n",
      "['Amelia\\tF\\t7235']\n",
      "['Harper\\tF\\t7179']\n",
      "['Lillian\\tF\\t7143']\n",
      "['Samantha\\tF\\t6913']\n",
      "['Evelyn\\tF\\t6869']\n",
      "['Victoria\\tF\\t6849']\n",
      "['Brooklyn\\tF\\t6769']\n",
      "['Zoe\\tF\\t6435']\n",
      "['Layla\\tF\\t6251']\n",
      "['Hailey\\tF\\t5901']\n",
      "['Leah\\tF\\t5758']\n",
      "['Kaylee\\tF\\t5602']\n",
      "['Anna\\tF\\t5600']\n",
      "['Aaliyah\\tF\\t5495']\n",
      "['Gabriella\\tF\\t5487']\n",
      "['Allison\\tF\\t5415']\n",
      "['Nevaeh\\tF\\t5356']\n",
      "['Alexis\\tF\\t5337']\n",
      "['Audrey\\tF\\t5291']\n",
      "['Savannah\\tF\\t5176']\n",
      "['Sarah\\tF\\t5167']\n",
      "['Alyssa\\tF\\t5078']\n",
      "['Claire\\tF\\t4941']\n",
      "['Taylor\\tF\\t4852']\n",
      "['Riley\\tF\\t4816']\n",
      "['Camila\\tF\\t4789']\n",
      "['Arianna\\tF\\t4708']\n",
      "['Ashley\\tF\\t4691']\n",
      "['Brianna\\tF\\t4624']\n",
      "['Sophie\\tF\\t4566']\n",
      "['Peyton\\tF\\t4464']\n",
      "['Bella\\tF\\t4332']\n",
      "['Khloe\\tF\\t4304']\n",
      "['Genesis\\tF\\t4292']\n",
      "['Alexa\\tF\\t4288']\n",
      "['Serenity\\tF\\t4209']\n",
      "['Kylie\\tF\\t4164']\n",
      "['Aubree\\tF\\t4039']\n",
      "['Scarlett\\tF\\t4039']\n",
      "['Stella\\tF\\t3994']\n",
      "['Maya\\tF\\t3961']\n",
      "['Katherine\\tF\\t3945']\n",
      "['Julia\\tF\\t3872']\n",
      "['Lucy\\tF\\t3803']\n",
      "['Madelyn\\tF\\t3791']\n",
      "['Autumn\\tF\\t3765']\n",
      "['Makayla\\tF\\t3760']\n",
      "['Kayla\\tF\\t3755']\n",
      "['Mackenzie\\tF\\t3750']\n",
      "['Lauren\\tF\\t3654']\n",
      "['Gianna\\tF\\t3615']\n",
      "['Ariana\\tF\\t3572']\n",
      "['Faith\\tF\\t3520']\n",
      "['Alexandra\\tF\\t3478']\n",
      "['Melanie\\tF\\t3462']\n",
      "['Sydney\\tF\\t3449']\n",
      "['Caroline\\tF\\t3412']\n",
      "['Naomi\\tF\\t3411']\n",
      "['Bailey\\tF\\t3399']\n",
      "['Morgan\\tF\\t3392']\n",
      "['Ellie\\tF\\t3382']\n",
      "['Kennedy\\tF\\t3379']\n",
      "['Jasmine\\tF\\t3367']\n",
      "['Eva\\tF\\t3355']\n",
      "['Skylar\\tF\\t3337']\n",
      "['Kimberly\\tF\\t3307']\n",
      "['Violet\\tF\\t3270']\n",
      "['Molly\\tF\\t3251']\n",
      "['Aria\\tF\\t3226']\n",
      "['Jocelyn\\tF\\t3224']\n",
      "['Trinity\\tF\\t3224']\n",
      "['London\\tF\\t3192']\n",
      "['Lydia\\tF\\t3160']\n",
      "['Madeline\\tF\\t3157']\n",
      "['Reagan\\tF\\t3083']\n",
      "['Piper\\tF\\t3056']\n",
      "['Annabelle\\tF\\t3046']\n",
      "['Andrea\\tF\\t3041']\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/sample_data/sample_data.txt','rU') as f:\n",
    "    csvreader = csv.reader(f,delimiter=',')\n",
    "    for row in csvreader:\n",
    "        print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it doesn't look the way we want. it has all these `\\t` characters. This is because we still told it that columns are delimited by a comma. Change the \"delimiter\" argument to \"\\t\" and see if that works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing csv files\n",
    "\n",
    "Now let's write our data to a file. First, read in the file sample_text.csv, but this time only read in the first 25 lines. You will have to modify the `for` loop to loop 25 times, and at each loop call `csvreader.next()` and append it to a list called `subdata`. The `next` method just reads the next line in the file. When it's done, print the contents of `subdata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subdata = []\n",
    "\n",
    "with open('../datasets/sample_data/sample_data.csv','r') as f:\n",
    "    #fill in the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now copy and paste your code from above and modify it slightly to save the contents of `subdata` to your own csv file. You will save the data into a csv file in **R:\\Student_Data\\your_duckid\\test_data.csv**. \n",
    "\n",
    "You will make 3 changes. \n",
    "* First, change the mode for `open` to `w`\n",
    "* Instead of csvreader, you will create a variable `csvwriter` using the `csv.writer` function\n",
    "* Loop through each row of `subdata`, and use the function `csvwriter.writerow(rowdata)`\n",
    "\n",
    "Open the file in Excel to confirm that it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files\n",
    "\n",
    "Now let's load the same data that's stored in a JSON file. First, open the file in Notepad++ to see how it's stored. Notice it looks a lot like how you would type it into Python! Now we will use the `json` package to load from that file into a Python variable. What is the datatype of `namedata`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../datasets/sample_data/sample_data.json','r') as f:\n",
    "    namedata = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON files are very useful for saving Python dictionary and list data. Let's use our contact list example from a previous lecture. Notice that `contacts` is a list, and each element in the list is a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person1 = {'Name' : 'John Q. Taxpayer', \n",
    "         'Phone' : '541-555-1234',\n",
    "         'Email' : 'johnq@yahoo.com'}\n",
    "\n",
    "person2 = {'Name' : 'Barack Obama', \n",
    "        'Email' : 'president@whitehouse.gov',\n",
    "        'Phone' : '555-123-4567'}\n",
    "\n",
    "person3 = {'Name': 'Jason Hubbard',\n",
    "          'Email': 'hubbard3@uoregon.edu',\n",
    "          'Phone': '555-123-7712'}\n",
    "\n",
    "contacts = [person1,person2,person3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save it into a JSON file under **R:\\Student_Data\\your_duckid\\test_data.json** using `json.dump`. Now open the file in Notepad++ and see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('R:\\Student_Data\\MYDUCKID\\test_data.json','w') as f:\n",
    "     json.dump(contacts,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Files\n",
    "\n",
    "Plain text is great because you can move your data to other programs easily. But sometimes you want to save your data for Python only. Plain text files are also not efficient for storing large amounts of information. So if you have lots of data that you know you only need in python, you can save it using the **pickle** package (yes, pickle). Files have the `.pkl` extension. Here we load in a .pkl file that has the same info as our contact list we created above. We save it as a new variable name, loaded_contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../datasets/sample_data/sample_data.pkl','r') as f:\n",
    "    loaded_contacts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the data in a similar way, just using `pickle.dump`. Here we just save a variable called `x` that's a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "\n",
    "with open('R:\\Student_Data\\MYDUCKID\\saved_x.pkl','w') as f:\n",
    "     pickle.dump(x,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now load the saved data, saving the result into a variable y\n",
    "with open('R:\\Student_Data\\MYDUCKID\\saved_x.pkl','r') as f:\n",
    "     y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to open your .pkl file in Notepad++. What does it look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
